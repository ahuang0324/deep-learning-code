{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0945b51-9412-46e4-917d-a6703dccb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934dea50-0be8-458e-ac30-06fc2ec4a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "\n",
    "features,labels = d2l.synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888a873-9707-4ec0-bf7a-9bfca15a5744",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164df940-1a58-44f5-ad4c-825c0186698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.utils.data.DataLoader 创建一个数据加载器对象。数据加载器将数据集包装起来，并提供迭代数据集的功能，\\n每次返回一个大小为 batch_size 的小批量。shuffle=is_train 参数控制是否在每个 epoch 之前随机打乱数据。\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(data_array,batch_size,is_train = True):\n",
    "    # is_train 表示在训练中，需要打乱数据\n",
    "    dataset = data.TensorDataset(*data_array)\n",
    "    \"\"\"\n",
    "    TensorDataset 类\n",
    "        功能：torch.utils.data.TensorDataset 是 Dataset 类的一个具体实现，用于处理张量形式的数据。\n",
    "        它可以将多个张量组合成一个数据集，每个张量的第一维必须相同，表示样本的数量。\n",
    "    \"\"\"\n",
    "    return data.DataLoader(dataset,batch_size,shuffle = is_train)\n",
    "\"\"\"\n",
    "torch.utils.data.DataLoader 创建一个数据加载器对象。数据加载器将数据集包装起来，并提供迭代数据集的功能，\n",
    "每次返回一个大小为 batch_size 的小批量。shuffle=is_train 参数控制是否在每个 epoch 之前随机打乱数据。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df0eb17-63ff-40f0-89e7-a3bf7db7f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array([features,labels],batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1157eed5-c1b5-49ac-8163-a100e5b880b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.6312, -0.6313],\n",
       "         [ 0.8496, -0.8005],\n",
       "         [ 0.2374,  0.3075],\n",
       "         [ 0.8989, -0.5394],\n",
       "         [-0.7739,  0.2213],\n",
       "         [ 0.1109,  0.3202],\n",
       "         [-0.4851,  0.0514],\n",
       "         [ 1.0042,  1.3578],\n",
       "         [ 2.4910,  1.5118],\n",
       "         [ 1.9617,  1.8128]]),\n",
       " tensor([[7.5994],\n",
       "         [8.6350],\n",
       "         [3.6251],\n",
       "         [7.8263],\n",
       "         [1.9030],\n",
       "         [3.3300],\n",
       "         [3.0388],\n",
       "         [1.5939],\n",
       "         [4.0414],\n",
       "         [1.9616]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataLoader 对象本身就是一个可迭代对象，内部有 __iter__ 方法\n",
    "使用iter() 就是调用了这个方法，显式的返回一个迭代器。\n",
    "也可以隐式的进行访问如\n",
    "for x,y in data_iter:  # for 循环会自动调用__iter__ \n",
    "    # 每次处理一小批数据\n",
    "\n",
    "你可以把 DataLoader 想象成一个可以生产迭代器的工厂 每次调用 iter(data_iter) 都会创建一个新的迭代器，用于遍历 DataLoader 中的数据。  如果你直接调用 next(data_iter)，Python 解释器会尝试将 DataLoader 对象本身当作迭代器来使用，\n",
    "但这是不行的，因为它没有实现 __next__ 方法（虽然它实现了 __iter__ 方法）。\n",
    "next() 函数需要一个迭代器作为参数。  DataLoader 对象本身虽然是可迭代的，但它不是一个迭代器对象。  你需要先通过 iter(data_iter) \n",
    "获取 DataLoader 的迭代器，然后再使用 next() 函数。\n",
    "\"\"\"\n",
    "\n",
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37046-a2fd-4f62-b93f-060d3d59d711",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c1f5aac-ae99-4fb1-b81d-3ea1bff94019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e877-a491-4bee-a72e-d6fa74b63f00",
   "metadata": {},
   "source": [
    "### torch.nn 库\n",
    "https://blog.csdn.net/HiWangWenBing/article/details/120614234 介绍\n",
    "- torch.nn.Liner(in_features,out_features,bias = True) 全连接\n",
    "   in_features 是输入的x的维度，也就是特征的个数，自己实现的时候，就是列的个数\n",
    "- torch.nn.functional nn.functional定义了创建神经网络所需要的一些常见的处理函数\n",
    "   如 nn.functional.sigmod nn.functional.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba82839-af3f-4c21-bb34-2bace4fc2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2,1))\n",
    "# Sequential 是神经网络每一层的容器，这里我们只有一层全连接\n",
    "# 输入特征大小为2 输出特征大小1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f297ee4-fcc6-4a73-9749-e595d5e684bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型参数\n",
    "net[0].weight.data.normal_(0,0.1)\n",
    "net[0].bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f58cc-920c-4e74-9b50-a48ec5eaccdf",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd2a34e-47e8-482a-83e7-d6eb4423d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a458f-a7f1-4adc-94c0-a0ba03e81ab3",
   "metadata": {},
   "source": [
    "### 定义优化函数 随机梯度下降\n",
    "\n",
    "torch.optim 模块包含了多种优化器类\n",
    "- torch.optim.SGD（随机梯度下降）\n",
    "- torch.optim.Adam Adam 是一种自适应学习率的优化算法，它结合了 AdaGrad 和 RMSProp 的优点，能够自适应地调整每个参数的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63964775-2649-4c68-aa34-f6bca735140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(),lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8f14e-ded8-4cac-a0c8-ca047e33a60f",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "训练过程：\n",
    "- 向前传播 net(x)\n",
    "- 计算损失 loss(net(x),y)\n",
    "- 梯度清零 trainer.zero_grad() 类似之前parama.grad.zero_()\n",
    "- 反向传播，梯度清零之后才能反向传播，因为是累加的 l.backward()\n",
    "- 更新参数 trainer.step()\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06341b7e-d6fc-4c59-8ce6-ef7aa2b9bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000319\n",
      "epoch 2, loss 0.000105\n",
      "epoch 3, loss 0.000106\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for x,y in data_iter:\n",
    "        l = loss(net(x),y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ff77-3c64-4e2a-8e51-d77f7a33d033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
