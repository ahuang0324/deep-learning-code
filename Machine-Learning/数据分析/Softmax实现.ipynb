{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c57687",
   "metadata": {},
   "source": [
    "### 首先是数据集\n",
    "MNIST 手写数据集太老了，可以选择Fashion-MNIST 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e3334",
   "metadata": {},
   "source": [
    "### 手写Softmax 分类问题的收获\n",
    "- 学习了 torchvision.transforms 的一些操作，主要是对图片数据的处理\n",
    "    - 例如 ToTensor() plt.imgshow()也能接受 tensor变量 还有使用管道对数据进行批量处理 transforms.Compose = [] Resize 调整大小啊\n",
    "- 大体了解了一下DataSet这个类，自己写MyDataSet 要继承这个基类，然后重写 __getitem__ __len__ 这两个函数 __getitem() 返回一个x，一个y\n",
    "- DataLoader(dataset = ,bitch_size= , shuffle = True,num_workers = )\n",
    "- 写softmax函数之前，详细了解了花式索引 y_hat[[0,1],[0,2]]\n",
    "- 会了一行代码完成交叉熵损失函数\n",
    "- 学会如何计算模型的准确率\n",
    "    - n个变量的累加器 Accumulator\n",
    "    - 使用bool列表 然后求和 求准确率 cmp = y_hat.type(y.dtype) == y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb909aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display() # 使用svg来显示图片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b789f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec987a23",
   "metadata": {},
   "source": [
    "### 定义我们的加载数据集函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "637c4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "def load_data_fashion_mnist(batch_size ,resize = None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root = '../data',train = True,transform = trans,download = True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root = '../data',train = False,transform = trans,download = True)\n",
    "    return (data.DataLoader(mnist_train,batch_size,shuffle = True,\n",
    "                num_workers = 4),\n",
    "            data.DataLoader(mnist_test,batch_size,shuffle = True,num_workers = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d71b3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter,test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5a33f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3098, 0.9137, 0.6706,  ..., 0.4824, 0.5216, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0118, 0.0000],\n",
      "          ...,\n",
      "          [0.0039, 0.2549, 0.3373,  ..., 0.4392, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), tensor([6, 7, 4, 6, 0, 6, 0, 9, 6, 8, 9, 0, 8, 9, 1, 5, 4, 2, 9, 0, 9, 6, 6, 2,\n",
      "        1, 2, 3, 4, 7, 1, 4, 4, 6, 3, 7, 6, 8, 1, 8, 8, 8, 8, 1, 2, 5, 6, 2, 7,\n",
      "        1, 7, 4, 3, 9, 0, 3, 4, 5, 9, 8, 0, 2, 0, 5, 9, 8, 6, 9, 2, 3, 7, 8, 6,\n",
      "        9, 4, 2, 0, 1, 2, 2, 3, 4, 3, 6, 4, 9, 7, 0, 0, 4, 9, 3, 3, 4, 1, 9, 7,\n",
      "        0, 3, 9, 0, 9, 3, 6, 6, 6, 4, 6, 1, 2, 9, 9, 7, 5, 6, 4, 7, 5, 7, 9, 8,\n",
      "        6, 0, 4, 7, 3, 4, 3, 4, 1, 5, 4, 3, 5, 7, 0, 5, 7, 3, 2, 0, 1, 2, 4, 9,\n",
      "        7, 4, 9, 4, 2, 6, 8, 0, 1, 5, 6, 8, 7, 0, 2, 5, 1, 8, 0, 7, 8, 8, 3, 2,\n",
      "        1, 8, 7, 4, 2, 6, 5, 3, 7, 9, 0, 3, 2, 5, 8, 5, 4, 3, 2, 2, 1, 0, 5, 3,\n",
      "        7, 2, 2, 4, 0, 4, 5, 9, 3, 6, 0, 5, 4, 1, 2, 4, 5, 0, 1, 9, 2, 6, 3, 6,\n",
      "        9, 3, 9, 5, 8, 2, 9, 5, 0, 2, 3, 4, 1, 4, 2, 0, 5, 8, 8, 2, 0, 5, 5, 1,\n",
      "        8, 5, 7, 1, 7, 5, 6, 1, 5, 9, 3, 5, 9, 7, 5, 7])]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45ba6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数 这是一个一层的网络结构\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10 # 10分类\n",
    "w = torch.normal(0,0.01,size = (num_inputs,num_outputs),requires_grad = True)\n",
    "b = torch.zeros(num_outputs,requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3249c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义softmax操作\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = torch.exp(x)\n",
    "    partition = x_exp.sum(dim = 1,keepdim = True)\n",
    "    # 这里为什么选择第一个维度，是因为 x的size 是 (batch_size,num_features)\n",
    "    # 也就是 (batch_size,10) 因为经过矩阵的乘法 (batch_size,28*28) * (28*28,10) 已经变成了(batch_sizem,10)\n",
    "    return x_exp / partition # 利用了广播机制\n",
    "    # partition的机构应该是 (batch_size,1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f2c44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络模型\n",
    "def net(x):\n",
    "    return softmax(torch.matmul(x.reshape(-1,w.shape[0]),w) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea32918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.1000, 0.5000]),\n",
       " tensor([[0.1000, 0.6000],\n",
       "         [0.3000, 0.5000]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算交叉熵时，通常会使用到 Tensor张量的一种特殊索引类型 花式索引 Fancy indexing\n",
    "# 也就是使用列表 给出下标来确定位置\n",
    "# 单维度\n",
    "# import torch\n",
    "x = torch.tensor([10, 20, 30, 40])\n",
    "indices = torch.tensor([0, 2])\n",
    "print(x[indices])  # 输出: tensor([10, 30])\n",
    "# 多维度\n",
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "\"\"\"\n",
    "y_hat[[0, 1], [0, 2]] 是在选择了第一行和第二行的基础上，\n",
    "对第一行选择第一列，第二行选择第二列   是在选择的基础上在进行选择\n",
    "y_hat[:,y] 利用了广播机制， 首先选了第一行和第二行，然后对两个行都选择第一列和第三列\n",
    "\"\"\"\n",
    "y_hat[[0, 1], y],y_hat[:,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c099df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义损失函数 交叉熵损失函数\n",
    "\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -torch.log(y_hat[range(len(y)),y])\n",
    "cross_entropy(y_hat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a48d4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算正确率 accuracy 计算当前这一批次 的准确率\n",
    "def accuracy(y_hat,y):\n",
    "    \"\"\"\n",
    "        传入格式 y_hat (batch_size,num_class)\n",
    "        第一维度是每个样本，第二维度是每个样本对每一类的预测\n",
    "        y 则 是(batch_size) 即每一个样本所属的类\n",
    "    \"\"\"\n",
    "    if len(y_hat) > 1 and y_hat.shape[1] > 1:\n",
    "        # 保证这个批次不只有一个样本，且class 大于1\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    # cmp 是一个bool 数组 相等的地方为true 其他未False\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "    # 返回预测正确的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "525bf439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4996120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self,n):\n",
    "        self.data = [0.0] * n\n",
    "    \n",
    "    def add(self,*args):\n",
    "        self.data = [a + float(b) for a,b in zip(self.data,args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e254ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估\n",
    "\"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "def evaluate_accuracy(net,data_iter):\n",
    "    # 传入网络和数据集\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval() # 将模型设置为评估模式\n",
    "\n",
    "    metric = Accumulator(2) # 表示有两个变量 正确的和总量\n",
    "\n",
    "    for x,y in data_iter:\n",
    "        metric.add(accuracy(net(x),y),y.numel())\n",
    "    \n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5d346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.087"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net,test_iter) # 算一个随机的初始化的精度\n",
    "# 正确率只有 8.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0191e6",
   "metadata": {},
   "source": [
    "### 手写Softmax 分类问题的收获\n",
    "- 学习了 torchvision.transforms 的一些操作，主要是对图片数据的处理\n",
    "    - 例如 ToTensor() plt.imgshow()也能接受 tensor变量 还有使用管道对数据进行批量处理 transforms.Compose = [] Resize 调整大小啊\n",
    "- 大体了解了一下DataSet这个类，自己写MyDataSet 要继承这个基类，然后重写 __getitem__ __len__ 这两个函数 __getitem() 返回一个x，一个y\n",
    "- DataLoader(dataset = ,bitch_size= , shuffle = True,num_workers = )\n",
    "- 写softmax函数之前，详细了解了花式索引 y_hat[[0,1],[0,2]]\n",
    "- 会了一行代码完成交叉熵损失函数\n",
    "- 学会如何计算模型的准确率\n",
    "    - n个变量的累加器 Accumulator\n",
    "    - 使用bool列表 然后求和 求准确率 cmp = y_hat.type(y.dtype) == y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6318c24",
   "metadata": {},
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2351cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520a252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
