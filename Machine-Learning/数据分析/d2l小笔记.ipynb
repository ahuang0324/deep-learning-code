{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd0c7d4",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "\n",
    "#### \"Regularization\"在英语中的基本含义是\"使规则化\"、\"使正常化\"或\"标准化\"的过程。在机器学习语境中，它通常译为\"正则化\"。\n",
    "#### Regularization 的核心思想是 “约束” 或 “惩罚”。它是一种用于 防止模型过度拟合 (Overfitting) 的技术。\n",
    "为了防止overfitting的技术，通过惩罚模型的复杂度来约束模型的学习能力  \n",
    "它会给模型增加一些“成本”，使得模型在追求低训练误差的同时，也会权衡模型本身的复杂度。\n",
    "- L1 正则化(Lasso Regression回归)\n",
    "    - 惩罚项为权重的绝对值之和（λ∑|w|）。\n",
    "    稀疏性 (Sparsity)：L1 正则化倾向于将一些模型参数（wi）推向零。这意味着 L1 正则化可以自动进行特征选择，将不重要的特征的权重置零，从而得到一个更稀疏的模型。\n",
    "    - L1 范数: 正则化项是模型权重绝对值之和（L1 范数）。\n",
    "- L2 正则化（Ridge Regression 岭回归）\n",
    "    - 正则化项: $ \\lambda\\sum{w^2}$\n",
    "    - 权重衰减 (Weight Decay)：L2 正则化倾向于将模型参数推向零，但通常不会将它们完全置零。它使得模型的权重更加平滑，减小了对单个特征的依赖。\n",
    "    - L2 范数: 正则化项是模型权重平方之和（L2 范数）。\n",
    "- Elastic Net 正则化 $ \\lambda\\sum{\\left| w_i\\right|} + \\lambda\\sum{w^2}$\n",
    "    - 结合了L1 和L2的优点\n",
    "- 其他形式的正则化（针对特定模型或情况）\n",
    "    - Dropout (在神经网络中)：在训练过程中，随机“丢弃”一部分神经元及其连接，迫使网络学习冗余的表示，提高鲁棒性。\n",
    "    - Batch Normalization (在神经网络中)：对每一层神经网络的输入进行归一化，可以加速训练，并且在一定程度上起到正则化作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf9f13",
   "metadata": {},
   "source": [
    "一些问题：  \n",
    "1.为什么L2正则化要叫岭回归：\n",
    "L2 正则化通过给模型系数的平方和增加惩罚项，使得在训练过程中，即使面对多重共线性，模型也不会赋予某些系数过大的值。这样做有效地**“压低”**了系数的大小，增加了估计的稳定性，这种“压低”的过程在可视化时（岭图）呈现出类似山脊的形状，因此得名“岭回归”  \n",
    "2.强制学习冗余表示 (Redundancy)\n",
    "减少神经元之间的协同适应： 在没有 Dropout 的情况下，神经元可能会形成高度协同的“共适应”关系，即某些神经元组合在一起才能正确地进行预测。  \n",
    "3. Dropout 强行打断了这种固定的共适应关系。\n",
    "每个神经元更独立地学习特征： 由于不知道哪些神经元会在下一次训练中被丢弃，每个神经元都需要学习对输入数据更具鲁棒性、更独立的特征。即使它所在的“子网络”中缺少某些其他神经元，它仍然能够提供有用的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6bb52",
   "metadata": {},
   "source": [
    "在神经网络中 L2正则会为什么要叫做权重衰减：  \n",
    "我们通过反向传播的过程来看（利用梯度更新W）\n",
    "原始的梯度下降更新是：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee0507",
   "metadata": {},
   "source": [
    "$$ w_{new} = w_{old} - \\alpha\\dfrac{\\sigma  J(\\theta)}{\\sigma w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee1649",
   "metadata": {},
   "source": [
    "加入L2正则化后的更新：  \n",
    "$$ w_{new} = w_{old} - \\alpha ( \\dfrac{\\sigma  J(\\theta)}{\\sigma w} + \\lambda w_{old})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397ff17",
   "metadata": {},
   "source": [
    "展开之后：  \n",
    "$$ w_{new} = w_{old}(1- \\alpha\\lambda) - \\alpha\\dfrac{\\sigma  J(\\theta)}{\\sigma w}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c89553",
   "metadata": {},
   "source": [
    "衰减项的贡献： \n",
    "(1−αλ) 这个项会使得权重在每次更新时都乘以一个小于 1 的系数。这就像是在每次迭代中，“衰减”了权重的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b545644",
   "metadata": {},
   "source": [
    "我认为的dropout 和L1正则化的一些相同点和区别：\n",
    "- dropout是每次随机丢弃每一层的一部分参数，或者说是选择网络中一个子网络进行训练\n",
    "- L1正则化则是加入惩罚 使参数趋向于0\n",
    "- 区别是正则化直接作用于参数 而dropout是间接作用于参数 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a35e2b",
   "metadata": {},
   "source": [
    "### nn.Module  \n",
    "核心有两大块：\n",
    "- 定义网络中有哪些元素， def __init__() ：模块会自动将器内部的nn.Moudle的实例内容r(nn.Linear、nn.Conv2d)如捕捉，进行统一的管理  \n",
    "nn.Module 实现了一个树形结构，每个模块可以包含多个子模块，形成层次结构：\n",
    "_modules: 字典，存储所有已注册的子模块  \n",
    "_parameters: 字典，存储所有已注册的参数  \n",
    "_buffers: 字典，存储所有已注册但不需要梯度的张量（如 BatchNorm 的 running_mean）  \n",
    "**内部工作原理**\n",
    "当你给 self 分配一个 nn.Module 子类的实例时，以下是幕后发生的事情：\n",
    "当你执行 self.conv1 = nn.Conv2d(3, 16, 3) 时，nn.Module 的 __setattr__ 方法被调用\n",
    "__setattr__ 检查你分配的对象是否为 nn.Module 的实例\n",
    "如果是，它将该模块添加到 self._modules 字典中，键为属性名（如 'conv1'）\n",
    "这使 PyTorch 能够：\n",
    "跟踪所有子模块\n",
    "递归地应用操作（如 .to(device)、.train()）\n",
    "收集所有参数进行优化\n",
    "- 定义网络中这些元素的连接方式，重写 def forward(self,x): pytorch会自动向前传播，  \n",
    "Module(x) 会自动调用moudle.forward(slef,x)\n",
    "- 其他的就是nn.Moudle 原本的功能\n",
    "    - Module.train() 将网络设置为训练模式\n",
    "    - Module.eval() 将模型设置为评估函数，不启动Dropout(保证输出的准确) 不会构建计算图，节省计算资源\n",
    "    - Module.parameters()\n",
    "    - Module.named_parameters() 返回名字和参数\n",
    "    - Module.to(device) 会递归的将所有参数和子模块移动到指定设备 注意返回的是副本，需要赋值给原来的module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cbe84",
   "metadata": {},
   "source": [
    "nn.Module 的属性注册工作流程\n",
    "当你创建一个 nn.Module 子类实例时，其 __init__ 方法被调用\n",
    "在 __init__ 中，你给 self 分配各种属性\n",
    "当分配属性时，nn.Module 的 __setattr__ 方法会：\n",
    "检查值是否是 Parameter，如是则添加到 _parameters\n",
    "检查值是否是 Module，如是则添加到 _modules\n",
    "检查值是否是 Tensor 且 persistent=True，如是则添加到 _buffers\n",
    "否则，正常设置属性\n",
    "这种机制确保了你的模型结构被正确跟踪，所有参数都可以被优化器访问，并且操作（如移动到 GPU）可以应用到整个模型。\n",
    "\n",
    "这就是为什么将层\"作为属性定义\"如此重要 - 这是 PyTorch 自动构建计算图和管理参数的关键机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fb9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class mymodule(nn.Module):\n",
    "    def __init__(self,num_inputs,hidden_size,num_outputs):\n",
    "        super(mymodule,self).__init__()\n",
    "        \"\"\"\n",
    "            super() 是Python内置函数，用于获取父类或兄弟类的临时代理对象，使你能够调用它们的方法。\n",
    "            super(type, obj) type: 一个类（通常是当前类） obj: 类的实例（通常是self）\n",
    "            其实也可以写成super(nn.Module,self) 效果是相同的，但是不符合python的惯例\n",
    "            python3 可以简写为 super().__init__()\n",
    "            注意 python不会自动调用父类的初始化函数，这与C++不同\n",
    "            Python 的核心设计原则之一是“显式优于隐式”\n",
    "        \"\"\"\n",
    "\n",
    "        self.fc1 = nn.Linear(num_inputs,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size,num_outputs)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.f1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.f2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb236088",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mymodule(28*28,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9f355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "参数实际上存储在 self.conv1 和 self.fc1 这些子模块中，而不是直接存储在根模块 MyNet 的 _parameters 中。\n",
    "\"\"\"\n",
    "\n",
    "net._parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdc5fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1', Linear(in_features=784, out_features=64, bias=True)),\n",
       "             ('relu', ReLU()),\n",
       "             ('fc2', Linear(in_features=64, out_features=10, bias=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([64, 784])\n",
      "fc1.bias: torch.Size([64])\n",
      "fc2.weight: torch.Size([10, 64])\n",
      "fc2.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 2. 使用 named_parameters() 方法（显示名称和参数）\n",
    "for name, param in net.named_parameters(): \n",
    "    # type(net.named_parameters()) 也是生成器\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters(): \n",
    "    # type(net.parameters()) = gernerator 会返回一个生成器\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bead58aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e348738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(net.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6eba2984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "next(net.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0f59c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌套模块\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        \n",
    "        # 子模块\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 14 * 14, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
