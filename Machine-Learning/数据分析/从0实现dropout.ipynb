{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2584cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9532e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(x,dropout):\n",
    "    assert 0<= dropout <= 1\n",
    "    if dropout == 1: # 全部丢弃\n",
    "        return torch.zeros_like(x)\n",
    "    elif dropout == 0:\n",
    "        return x # 不丢弃\n",
    "    mask = (torch.randn(x.shape) > dropout).float()\n",
    "\n",
    "    # 也相当于是一个bool数组，随机初始化一个形状和x一样的数组，然后大于dropout为1 小于为0\n",
    "    return mask * x / (1.0 - dropout)\n",
    "    # 乘法比bool索引要更快一点 x[mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefcbf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乘法版本的平均执行时间 (100 次运行): 0.008449 秒\n",
      "布尔索引版本的平均执行时间 (100 次运行): 0.019245 秒\n",
      "\n",
      "验证结果的一致性:\n",
      "乘法结果形状: torch.Size([1000, 1000])\n",
      "布尔索引结果形状: torch.Size([1000, 1000])\n",
      "数值是否接近 (对于相同 mask): True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "# 定义 dropout_layer 函数（稍微修改一下，使其能接受布尔索引版本）\n",
    "def dropout_layer_multiply(x, dropout):\n",
    "    \"\"\"使用乘法进行 dropout\"\"\"\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(x)\n",
    "    elif dropout == 0:\n",
    "        return x\n",
    "    mask = (torch.randn(x.shape) > dropout).float()\n",
    "    return mask * x / (1.0 - dropout)\n",
    "\n",
    "def dropout_layer_bool_index(x, dropout):\n",
    "    \"\"\"使用布尔索引进行 dropout\"\"\"\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(x)\n",
    "    elif dropout == 0:\n",
    "        return x\n",
    "    mask = (torch.randn(x.shape) > dropout).float()\n",
    "    # 注意：布尔索引会直接移除被 mask 为 0 的元素，\n",
    "    # 为了模拟乘法操作的输出形状，我们需要重新构建一个相同形状的张量\n",
    "    # 这是一个关键的差异，直接的布尔索引是改变形状的\n",
    "    # 为了公平比较，我们这里假设目标是得到一个与输入 x 形状相同的张量\n",
    "    # 实际应用中，如果 dropout != 0 and dropout != 1，\n",
    "    # 乘法版本的输出形状和输入形状一致，而布尔索引的输出形状会变小。\n",
    "    # 为了让测试更具可比性，这里我们模拟一种场景，即需要保持原形状。\n",
    "    # 一个常见的实现是先创建全零张量，然后用布尔索引填充。\n",
    "    # 但这样做会引入额外的操作，可能掩盖原始布尔索引的性能。\n",
    "    #\n",
    "    # 让我们采取一个更直接的比较，即比较生成 mask 后，\n",
    "    # 如何应用这个 mask 来“丢弃”元素，并保持原形状。\n",
    "\n",
    "    # 另一种更接近乘法操作的布尔索引模拟：\n",
    "    # 创建一个与 x 形状相同的全零张量\n",
    "    y = torch.zeros_like(x)\n",
    "    # 使用布尔索引将对应位置的 x 的值赋给 y\n",
    "    y[mask.bool()] = x[mask.bool()]\n",
    "    # 然后进行归一化\n",
    "    return y / (1.0 - dropout)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 准备测试数据\n",
    "torch.manual_seed(42) # 为了结果的可复现性\n",
    "data_size = (1000, 1000) # 可以根据需要调整张量的大小\n",
    "x_tensor = torch.randn(*data_size)\n",
    "dropout_rate = 0.5 # 0.5 表示丢弃 50% 的元素\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 设置 timeit 的参数\n",
    "number_of_runs = 100 # 执行测试的次数\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 测试乘法版本\n",
    "setup_multiply = \"\"\"\n",
    "import torch\n",
    "from __main__ import dropout_layer_multiply\n",
    "x = torch.randn({size}, device='cpu') # 在 CPU 上测试，以避免 GPU 调度开销\n",
    "dropout = {dropout}\n",
    "\"\"\".format(size=data_size, dropout=dropout_rate)\n",
    "\n",
    "time_multiply = timeit.timeit(\n",
    "    \"dropout_layer_multiply(x, dropout)\",\n",
    "    setup=setup_multiply,\n",
    "    number=number_of_runs\n",
    ")\n",
    "\n",
    "print(f\"乘法版本的平均执行时间 ({number_of_runs} 次运行): {time_multiply / number_of_runs:.6f} 秒\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 测试布尔索引版本\n",
    "setup_bool_index = \"\"\"\n",
    "import torch\n",
    "from __main__ import dropout_layer_bool_index\n",
    "x = torch.randn({size}, device='cpu') # 在 CPU 上测试\n",
    "dropout = {dropout}\n",
    "\"\"\".format(size=data_size, dropout=dropout_rate)\n",
    "\n",
    "time_bool_index = timeit.timeit(\n",
    "    \"dropout_layer_bool_index(x, dropout)\",\n",
    "    setup=setup_bool_index,\n",
    "    number=number_of_runs\n",
    ")\n",
    "\n",
    "print(f\"布尔索引版本的平均执行时间 ({number_of_runs} 次运行): {time_bool_index / number_of_runs:.6f} 秒\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 验证结果是否一致（理论上应该非常接近，但由于随机性可能略有差异）\n",
    "print(\"\\n验证结果的一致性:\")\n",
    "result_multiply = dropout_layer_multiply(x_tensor, dropout_rate)\n",
    "result_bool_index = dropout_layer_bool_index(x_tensor, dropout_rate)\n",
    "\n",
    "# 检查形状\n",
    "print(f\"乘法结果形状: {result_multiply.shape}\")\n",
    "print(f\"布尔索引结果形状: {result_bool_index.shape}\")\n",
    "\n",
    "# 检查数值是否接近\n",
    "# 由于 mask 是随机生成的，每次运行 mask 都会不同，\n",
    "# 所以我们不能直接比较两个函数调用结果的数值。\n",
    "# 我们需要比较的是，对于**相同的随机 mask**，两种方法的结果是否一致。\n",
    "\n",
    "# 重新生成一个相同的 mask\n",
    "torch.manual_seed(42) # 重新设置种子以确保 mask 一致\n",
    "x_test = torch.randn(*data_size)\n",
    "mask_test = (torch.randn(x_test.shape) > dropout_rate).float()\n",
    "\n",
    "# 重新实现一次乘法和布尔索引，使用相同的 mask\n",
    "def dropout_layer_multiply_with_mask(x, mask, dropout):\n",
    "    return mask * x / (1.0 - dropout)\n",
    "\n",
    "def dropout_layer_bool_index_with_mask(x, mask, dropout):\n",
    "    y = torch.zeros_like(x)\n",
    "    y[mask.bool()] = x[mask.bool()]\n",
    "    return y / (1.0 - dropout)\n",
    "\n",
    "result_multiply_fixed_mask = dropout_layer_multiply_with_mask(x_test, mask_test, dropout_rate)\n",
    "result_bool_index_fixed_mask = dropout_layer_bool_index_with_mask(x_test, mask_test, dropout_rate)\n",
    "\n",
    "# 比较数值\n",
    "print(f\"数值是否接近 (对于相同 mask): {torch.allclose(result_multiply_fixed_mask, result_bool_index_fixed_mask)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1802e10",
   "metadata": {},
   "source": [
    "y = torch.zeros_like(x)\n",
    "使用布尔索引将对应位置的 x 的值赋给 y\n",
    "y[mask.bool()] = x[mask.bool()]\n",
    "\n",
    "对于这段代码，有bool索引的知识点，bool索引会打算原来tensor的形状，只返回一个1维数组，哪些元素符合条件\n",
    "返回的是一个副本，不是引用  \n",
    "但是当bool索引出现在 赋值符号（=）左侧时，是另外一种情况  \n",
    "这部分是赋值目标的指定。\n",
    "当布尔索引出现在赋值操作的左侧时，它并不创建一个新的张量。\n",
    "相反，它标识了目标张量 y 中的哪些位置应该被更新。\n",
    "mask.bool() 在这里的作用是告诉 PyTorch：“请找到 y 中所有在 mask.bool() 中对应值为 True 的位置。”\n",
    "PyTorch 会为这些被 True 标记的位置准备好接收新值的空间。  \n",
    "赋值操作的目的是将等号右侧的值，写入到等号左侧指定的位置。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a40612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数 或者说模型的参数\n",
    "input_size = 28*28 \n",
    "output_size = 10\n",
    "hidden1_size = 256\n",
    "hidden2_size = 256\n",
    "\n",
    "# 定义每一层dropout的概率\n",
    "dropout1 = 0.2\n",
    "dropout2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5419140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import is_\n",
    "from tarfile import is_tarfile\n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden1_size,hidden2_size,is_training = True):\n",
    "        super().__init__()\n",
    "        self.is_training = is_training\n",
    "        self.fc1 = nn.Linear(input_size,hidden1_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size,hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.is_tarining:\n",
    "            out = dropout_layer(out,dropout1)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.is_training:\n",
    "            out = dropout_layer(out ,dropout2)\n",
    "        # 交叉熵损失函数会自动计算softmax  所以我们这不需要最后经过softmax\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e398495",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size,output_size,hidden1_size,hidden2_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
